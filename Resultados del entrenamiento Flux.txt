julia> using Flux

julia> using Images

julia> using Colors

julia> using FileIO

julia> using Glob

julia> using Statistics

julia> using Random

julia>

julia> function load_images_and_labels(dataset_dir::String, classes::Vector{String})
           image_files = []
           labels = []

           for class in classes
               class_dir = joinpath(dataset_dir, class)
               for file in readdir(class_dir, join=true)
                   if endswith(file, ".jpg")
                       img = load(file)
                       img_gray = Gray.(img)
                       img_resized = imresize(img_gray, (128, 128))  # Tamaño cambiado a 128x128
                                       img_array = Float32.(img_resized) / 255.0  # Normalización de 0 a 1
                       push!(image_files, img_array)
                       push!(labels, class)
                   end
               end
           end

           num_images = length(image_files)
           img_height, img_width = size(image_files[1])
           images = hcat([reshape(img, img_height * img_width) for img in image_files]...)
           labels_onehot = Flux.onehotbatch(labels, classes)

           return images, labels_onehot
       end
load_images_and_labels (generic function with 1 method)

julia>

julia> dataset_dir = "C:\\Users\\Jhamil\\Desktop\\Dataset2mil - copia"
"C:\\Users\\Jhamil\\Desktop\\Dataset2mil - copia"

julia> classes = ["AmorSeco", "Boldo", "Charanguillo", "Ortiga", "Perejil"]
5-element Vector{String}:
 "AmorSeco"
 "Boldo"
 "Charanguillo"
 "Ortiga"
 "Perejil"

julia> train_x, train_y = load_images_and_labels(dataset_dir, classes)
([0.001261053482691447 0.0010149942893607944 … 0.0024913495662165623 0.0010611303881102917; 0.001261053482691447 0.0012918108818577784 … 0.0024913495662165623 0.0010611303881102917; … ; 0.002737408759547215 0.0018608227664349125 … 0.0010611303881102917 0.0024913495662165623; 0.002860438356212541 0.0021991542741364124 … 0.0010611303881102917 0.0024913495662165623], Bool[1 1 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 1 1])

julia>

julia> # Crear un modelo MLP con Flux

julia> model = Chain(
           Dense(128*128, 512, relu),
           Dropout(0.5),
           Dense(512, 256, relu),
           Dropout(0.5),
               Dense(256, length(classes)),
           softmax
       ) |> gpu  # Usar GPU si está disponible
Chain(
  Dense(16384 => 512, relu),            # 8_389_120 parameters
  Dropout(0.5),
  Dense(512 => 256, relu),              # 131_328 parameters
  Dropout(0.5),
  Dense(256 => 5),                      # 1_285 parameters
  NNlib.softmax,
)                   # Total: 6 arrays, 8_521_733 parameters, 32.508 MiB.

julia>

julia> # Definir la función de pérdida y optimizador

julia> function loss(x, y)
           pred = model(x)
           Flux.crossentropy(pred, y)
       end
loss (generic function with 2 methods)

julia>

julia> optimizer = Flux.Optimise.ADAM(0.001)
Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())

julia>

julia> # Dividir los datos en entrenamiento y validación

julia> num_samples = size(train_x, 2)
2000

julia> indices = shuffle(1:num_samples)
2000-element Vector{Int64}:
  415
 1858
   13
 1376
 1738
 1380
 1331
 1791
 1024
  742
 1145
 1529
  145
    ⋮
 1976
  856
 1688
 1872
  908
  845
 1862
 1496
 1048
 1487
 1737
 1056

julia> split_idx = round(Int, 0.8 * num_samples)
1600

julia> train_indices = indices[1:split_idx]
1600-element Vector{Int64}:
  415
 1858
   13
 1376
 1738
 1380
 1331
 1791
 1024
  742
 1145
 1529
  145
    ⋮
 1288
 1046
  650
 1270
 1150
  698
 1981
 1609
 1768
   16
 1087
  744

julia> val_indices = indices[split_idx+1:end]
400-element Vector{Int64}:
  257
  525
  751
 1864
 1454
 1855
  884
 1874
 1011
 1289
 1662
  127
 1374
    ⋮
 1976
  856
 1688
 1872
  908
  845
 1862
 1496
 1048
 1487
 1737
 1056

julia>

julia> train_x_data = train_x[:, train_indices]
16384×1600 Matrix{Float64}:
 0.000753556  0.00179931  0.00124567   0.00284506  0.00201461  …  0.00227605  0.00152249   0.0016609   0.000676663
 0.00083045   0.0018762   0.00192234   0.00279892  0.00212226     0.00230681  0.00192234   0.00163014  0.000661284
 0.000876586  0.00196847  0.00170704   0.00270665  0.00204537     0.00230681  0.00172241   0.00158401  0.000538255
 0.000861207  0.00198385  0.00196847   0.00261438  0.00193772     0.00226067  0.00169166   0.00153787  0.000584391
 0.000845829  0.00199923  0.00206075   0.00250673  0.00202999     0.00221453  0.00167628   0.00147636  0.00101499
 0.000876586  0.00206075  0.00190696   0.00242983  0.00210688  …  0.00222991  0.00158401   0.00141484  0.00141484
 0.000891965  0.00219915  0.00159938   0.00235294  0.00204537     0.00232218  0.00179931   0.0013687   0.00152249
 0.000876586  0.00230681  0.00189158   0.00230681  0.00196847     0.00241446  0.00159938   0.00135333  0.00147636
 0.000876586  0.00229143  0.00195309   0.00232218  0.00190696     0.00233756  0.00198385   0.00130719  0.00139946
 0.000753556  0.00213764  0.00193772   0.00230681  0.00183007     0.00241446  0.00256824   0.0013687   0.00146098
 0.000692042  0.00198385  0.00198385   0.00253749  0.00190696  …  0.00241446  0.00273741   0.00150711  0.00143022
 0.000753556  0.0018762   0.0020915    0.00301423  0.00198385     0.00227605  0.00233756   0.00169166  0.0014456
 0.00083045   0.00178393  0.00218378   0.0033218   0.0018762      0.00218378  0.00186082   0.0018762   0.00156863
 ⋮                                                             ⋱
 0.00101499   0.00175317  0.00221453   0.00339869  0.00230681     0.00127643  0.00153787   0.00193772  0.00103037
 0.00110727   0.00173779  0.00244521   0.00324491  0.00232218     0.00126105  0.00313725   0.00155325  0.00104575
 0.000938101  0.00169166  0.00133795   0.00275279  0.00233756     0.00121492  0.00236832   0.00149173  0.00106113
 0.000738178  0.00163014  0.000845829  0.00227605  0.00236832  …  0.00107651  0.00150711   0.0016609   0.00106113
 0.00070742   0.00158401  0.000891965  0.00232218  0.00233756     0.00113802  0.000630527  0.00175317  0.00112265
 0.000538255  0.00183007  0.000953479  0.00252211  0.00239908     0.00107651  0.000676663  0.00170704  0.00107651
 0.000630527  0.00170704  0.00104575   0.00269127  0.00242983     0.00112265  0.000415225  0.00161476  0.00107651
 0.00070742   0.00156863  0.00132257   0.00273741  0.00224529     0.00132257  0.000322953  0.00158401  0.00110727
 0.000445982  0.00161476  0.00129181   0.00278354  0.00201461  …  0.00153787  0.000292195  0.00163014  0.00106113
 0.000322953  0.0013687   0.00119954   0.00286044  0.00198385     0.00164552  0.000369089  0.00164552  0.000968858
 0.000507497  0.00153787  0.00126105   0.00290657  0.00195309     0.00164552  0.000461361  0.00176855  0.000968858
 0.000661284  0.00230681  0.000938101  0.0028912   0.00179931     0.00161476  9.22722e-5   0.00199923  0.00103037

julia> train_y_data = train_y[:, train_indices]
5×1600 OneHotMatrix(::Vector{UInt32}) with eltype Bool:
 ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  …  ⋅  ⋅  1  1  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅
 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  1
 ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅
 ⋅  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅
 ⋅  1  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  1  1  ⋅  1     ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  1  1  ⋅  ⋅  ⋅

julia> val_x_data = train_x[:, val_indices]
16384×400 Matrix{Float64}:
 0.00344483  0.000246059  0.000845829  0.00246059  0.0020915   …  0.000630527  0.00149173   0.00179931  0.00124567
 0.00344483  0.000261438  0.00083045   0.00230681  0.00213764     0.00047674   0.00127643   0.00195309  0.00249135
 0.00346021  0.000276817  0.000784314  0.00213764  0.00206075     0.00035371   0.00106113   0.00198385  0.00301423
 0.00347559  0.000307574  0.000738178  0.00206075  0.00198385     0.000292195  0.00107651   0.00201461  0.00299885
 0.00350634  0.000338331  0.000692042  0.00207612  0.00210688     0.000261438  0.00130719   0.00224529  0.00183007
 0.00352172  0.000338331  0.000661284  0.00212226  0.00235294  …  0.00035371   0.00153787   0.00242983  0.000768935
 0.0035371   0.000292195  0.000615148  0.00210688  0.0023837      0.00035371   0.00163014   0.00239908  0.0014456
 0.0035371   0.000246059  0.000599769  0.00207612  0.00222991     0.000246059  0.00163014   0.00233756  0.00195309
 0.00352172  0.000276817  0.000599769  0.0020915   0.00204537     0.000307574  0.00152249   0.00236832  0.000891965
 0.00352172  0.000322953  0.000584391  0.00212226  0.00199923     0.000307574  0.00147636   0.00233756  0.00133795
 0.00352172  0.000338331  0.000538255  0.00218378  0.00213764  …  0.000307574  0.00129181   0.00232218  0.00139946
 0.00352172  0.00035371   0.000492118  0.00224529  0.00229143     0.000322953  0.00130719   0.00230681  0.00178393
 0.00355248  0.000461361  0.000461361  0.00230681  0.0021684      0.000322953  0.00175317   0.00229143  0.00258362
 ⋮                                                             ⋱
 0.00193772  0.00132257   0.00146098   0.00261438  0.00184544     0.00199923   0.00118416   0.00198385  0.00261438
 0.00189158  0.00132257   0.0018762    0.00244521  0.00196847     0.00143022   0.00112265   0.00190696  0.00267589
 0.0018762   0.00132257   0.00246059   0.00226067  0.00199923     0.000815071  0.000861207  0.00183007  0.00253749
 0.0018762   0.00132257   0.00282968   0.00212226  0.00195309  …  0.000753556  0.000584391  0.00190696  0.00226067
 0.00201461  0.00133795   0.00282968   0.00192234  0.00196847     0.0013687    0.000630527  0.00196847  0.00215302
 0.00210688  0.00138408   0.00287582   0.00181469  0.0020915      0.00118416   0.000599769  0.00204537  0.00219915
 0.00213764  0.00146098   0.00290657   0.00173779  0.00202999     0.000845829  0.000615148  0.00210688  0.00222991
 0.00206075  0.00153787   0.0028912    0.00173779  0.00181469     0.000461361  0.000630527  0.00202999  0.00226067
 0.00199923  0.00156863   0.00282968   0.00170704  0.00179931  …  0.000722799  0.000538255  0.00193772  0.00219915
 0.00201461  0.00156863   0.00279892   0.00164552  0.00206075     0.000845829  0.000492118  0.00204537  0.00202999
 0.00201461  0.00153787   0.0028143    0.00164552  0.00222991     0.000492118  0.000661284  0.00212226  0.00206075
 0.00198385  0.00150711   0.00284506   0.00170704  0.00219915     0.00070742   0.000922722  0.00201461  0.00232218

julia> val_y_data = train_y[:, val_indices]
5×400 OneHotMatrix(::Vector{UInt32}) with eltype Bool:
 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅
 ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅
 ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  1  ⋅  ⋅  1  ⋅  ⋅  1
 ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  1  1  1  1     ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅
 ⋅  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  1  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅

julia>

julia> # Preparar los datos para el entrenamiento

julia> train_data = [(train_x_data[:, i], train_y_data[:, i]) for i in 1:size(train_x_data, 2)]
1600-element Vector{Tuple{Vector{Float64}, OneHotArrays.OneHotVector{UInt32}}}:
 ([0.0007535563380110498, 0.0008304498359268786, 0.000876585934676376, 0.0008612072350932102, 0.0008458285355100445, 0.000876585934676376, 0.0008919646342595418, 0.000876585934676376, 0.000876585934676376, 0.0007535563380110498  …  0.0009381007330090392, 0.0007381776384278839, 0.0007074202392615524, 0.0005382545438467288, 0.0006305267413457235, 0.0007074202392615524, 0.0004459823171297709, 0.0003229527204644446, 0.0005074971446803972, 0.0006612841405120551], [0, 1, 0, 0, 0])
 ([0.0017993079681022494, 0.0018762014660180784, 0.0019684737803889255, 0.0019838524799720915, 0.001999231179555257, 0.0020607459778879205, 0.0021991542741364124, 0.002306805171218573, 0.002291426471635407, 0.002137639475803749  …  0.001691657071020089, 0.0016301422726874258, 0.0015840061739379285, 0.001830065367268581, 0.0017070357706032548, 0.0015686274743547627, 0.00161476357310426, 0.0013687043797736074, 0.001537870075188431, 0.002306805171218573], [0, 0, 0, 0, 1])
 ([0.0012456747831082811, 0.0019223375647675757, 0.0017070357706032548, 0.0019684737803889255, 0.0020607459778879205, 0.0019069588651844098, 0.0015993848735210941, 0.0018915801656012442, 0.0019530949639339074, 0.0019377162643507415  …  0.001337946980607276, 0.0008458285355100445, 0.0008919646342595418, 0.0009534794325922049, 0.0010457516885271259, 0.0013225682810241101, 0.0012918108818577784, 0.0011995386843587838, 0.001261053482691447, 0.0009381007330090392], [1, 0, 0, 0, 0])
 ([0.0028450596566293755, 0.002798923557879878, 0.002706651360380883, 0.0026143791628818885, 0.002506728265799728, 0.0024298347678838993, 0.0023529412699680704, 0.002306805171218573, 0.0023221838708017387, 0.002306805171218573  …  0.002752787459130381, 0.0022760477720522414, 0.0023221838708017387, 0.002522106965382894, 0.0026912726607977175, 0.002737408759547215, 0.002783544858296712, 0.002860438356212541, 0.0029065744549620384, 0.0028911957553788728], [0, 0, 0, 1, 0])
 ([0.0020146098791384228, 0.0021222607762205834, 0.0020453672783047544, 0.0019377162643507415, 0.002029988578721589, 0.002106882076637418, 0.0020453672783047544, 0.0019684737803889255, 0.0019069588651844098, 0.001830065367268581  …  0.0023375625703849043, 0.002368319969551236, 0.0023375625703849043, 0.0023990773687175677, 0.0024298347678838993, 0.0022452903728859097, 0.0020146098791384228, 0.0019838524799720915, 0.0019530949639339074, 0.0017993079681022494], [0, 0, 0, 0, 1])
 ([0.0028911957553788728, 0.0022606690724690753, 0.0016301422726874258, 0.0014763552768557679, 0.0016762783714369231, 0.0020453672783047544, 0.002522106965382894, 0.0029219531545452045, 0.0031833910474590226, 0.0031372549487095253  …  0.002106882076637418, 0.0018915801656012442, 0.0017839292685190836, 0.002137639475803749, 0.002522106965382894, 0.0026297578624650546, 0.0023836986691344016, 0.0020607459778879205, 0.0019684737803889255, 0.002029988578721589], [0, 0, 0, 1, 0])
 ([0.0029527105537115357, 0.0029834679528778674, 0.002875817055795707, 0.0029988466524610334, 0.0032602845453748516, 0.0031218762491263597, 0.0029834679528778674, 0.0031987697470421883, 0.003168012347875857, 0.0030449827512105307  …  0.0009842368901944627, 0.0009996155897776286, 0.0011534025856092865, 0.0012302960835251153, 0.0012918108818577784, 0.0011380238860261207, 0.0009073433338427076, 0.0009381007330090392, 0.0010918877872766234, 0.0011226451864429548], [0, 0, 0, 1, 0])
 ([0.0017377931697695863, 0.001830065367268581, 0.0019684737803889255, 0.0020453672783047544, 0.0020146098791384228, 0.0018454440668517467, 0.00161476357310426, 0.0014609765772726022, 0.0015993848735210941, 0.0017070357706032548  …  0.002029988578721589, 0.0021222607762205834, 0.002153018175386915, 0.002153018175386915, 0.0021837755745532468, 0.002229911673302744, 0.002229911673302744, 0.0022452903728859097, 0.0023221838708017387, 0.0024298347678838993], [0, 0, 0, 0, 1])
 ([0.0023990773687175677, 0.002368319969551236, 0.0020453672783047544, 0.002137639475803749, 0.0021991542741364124, 0.0019684737803889255, 0.0020607459778879205, 0.002229911673302744, 0.0025374856649660596, 0.0026297578624650546  …  0.001261053482691447, 0.001337946980607276, 0.0020915033770542517, 0.0022760477720522414, 0.0022606690724690753, 0.0020453672783047544, 0.0019838524799720915, 0.0019838524799720915, 0.0017070357706032548, 0.0012918108818577784], [0, 0, 1, 0, 0])
 ([0.00021530180877330255, 0.00021530180877330255, 0.00021530180877330255, 0.00023068050835646834, 0.00023068050835646834, 0.0002460592225486157, 0.0002460592225486157, 0.0002460592225486157, 0.0002460592225486157, 0.0002460592225486157  …  0.0009534794325922049, 0.0008919646342595418, 0.0010765090876934575, 0.0014455978776894364, 0.001537870075188431, 0.0010149942893607944, 0.0009073433338427076, 0.001107266486859789, 0.0010149942893607944, 0.0011226451864429548], [0, 1, 0, 0, 0])
 ([0.0006920415396783866, 0.0004306036175466051, 0.0005382545438467288, 0.0008150711363437129, 0.0009381007330090392, 0.0011226451864429548, 0.001107266486859789, 0.0007535563380110498, 0.0010611303881102917, 0.0007381776384278839  …  0.0009996155897776286, 0.001107266486859789, 0.0008919646342595418, 0.0009996155897776286, 0.0011226451864429548, 0.0010918877872766234, 0.0009381007330090392, 0.0008150711363437129, 0.0008919646342595418, 0.0010457516885271259], [0, 0, 1, 0, 0])
 ([3.075740281857696e-5, 0.00019992310919013677, 0.00036908881921394196, 0.00038446751879710775, 0.0002460592225486157, 9.227220480348549e-5, 1.537870140928848e-5, 1.537870140928848e-5, 0.0, 0.00010765090438665128  …  0.0, 1.537870140928848e-5, 0.00010765090438665128, 1.537870140928848e-5, 9.227220480348549e-5, 0.0004921184450972314, 0.0009381007330090392, 0.0009688581321753708, 0.0005536332434298945, 0.00010765090438665128], [0, 0, 0, 1, 0])
 ([0.0005382545438467288, 0.0015532487747715968, 0.0011534025856092865, 0.001753171869352752, 0.0015993848735210941, 0.0014609765772726022, 0.001107266486859789, 0.0007689350375942155, 0.0018454440668517467, 0.0021683968749700807  …  0.0024759708666333966, 0.0020607459778879205, 0.00161476357310426, 0.002106882076637418, 0.0015993848735210941, 0.001753171869352752, 0.002368319969551236, 0.0021683968749700807, 0.0017839292685190836, 0.001337946980607276], [1, 0, 0, 0, 0])
 ⋮
 ([0.0023836986691344016, 0.0023990773687175677, 0.0024144560683007333, 0.0024298347678838993, 0.0024298347678838993, 0.0024144560683007333, 0.0023836986691344016, 0.002368319969551236, 0.0023836986691344016, 0.002291426471635407  …  0.0029988466524610334, 0.002860438356212541, 0.0029527105537115357, 0.0029834679528778674, 0.003091118849960028, 0.0032602845453748516, 0.0033679354424570123, 0.0033679354424570123, 0.0033371780432906806, 0.0033217993437075146], [0, 0, 0, 1, 0])
 ([0.0017377931697695863, 0.001830065367268581, 0.002737408759547215, 0.0036293733353708305, 0.0030449827512105307, 0.002137639475803749, 0.0021222607762205834, 0.002153018175386915, 0.0019530949639339074, 0.002106882076637418  …  0.0016455209722705917, 0.0016455209722705917, 0.001768550568935918, 0.0016608996718537573, 0.0017224144701864204, 0.001768550568935918, 0.0015993848735210941, 0.0014917339764389337, 0.0016301422726874258, 0.001830065367268581], [0, 0, 1, 0, 0])
 ([0.0005690119430130603, 0.0005843906425962261, 0.0005843906425962261, 0.0005997693421793919, 0.0005997693421793919, 0.0006151480417625576, 0.0006151480417625576, 0.0006305267413457235, 0.0006151480417625576, 0.0005843906425962261  …  0.0013225682810241101, 0.00161476357310426, 0.0015840061739379285, 0.0016301422726874258, 0.0016455209722705917, 0.0016455209722705917, 0.0015686274743547627, 0.0012764321822746128, 0.0009996155897776286, 0.0009534794325922049], [0, 1, 0, 0, 0])
 ([0.0012764321822746128, 0.0013071895814409443, 0.0013225682810241101, 0.0013071895814409443, 0.0013071895814409443, 0.0014302191781062706, 0.0016301422726874258, 0.0017993079681022494, 0.0020146098791384228, 0.0020453672783047544  …  0.0031833910474590226, 0.003014225352044199, 0.0027681661587135465, 0.002722030059964049, 0.002599000463298723, 0.002583621763715557, 0.002722030059964049, 0.002814302257463044, 0.0028296809570462094, 0.0029065744549620384], [0, 0, 0, 1, 0])
 ([0.002106882076637418, 0.002675893961214552, 0.002153018175386915, 0.0014917339764389337, 0.0009688581321753708, 0.0005997693421793919, 0.0011995386843587838, 0.0017993079681022494, 0.0011687812851924521, 0.0009381007330090392  …  0.0011687812851924521, 0.0015532487747715968, 0.0018146866676854152, 0.0018915801656012442, 0.0018762014660180784, 0.0017070357706032548, 0.00161476357310426, 0.0017070357706032548, 0.0018762014660180784, 0.0019684737803889255], [0, 0, 1, 0, 0])
 ([0.0030603614507936964, 0.0029219531545452045, 0.002722030059964049, 0.002583621763715557, 0.0023990773687175677, 0.0021683968749700807, 0.002306805171218573, 0.002706651360380883, 0.0026912726607977175, 0.002752787459130381  …  0.00026143792213178147, 0.00026143792213178147, 0.00026143792213178147, 0.00026143792213178147, 0.0002460592225486157, 0.0002460592225486157, 0.0002460592225486157, 0.00023068050835646834, 0.00023068050835646834, 0.00023068050835646834], [0, 1, 0, 0, 0])
 ([0.002798923557879878, 0.0028296809570462094, 0.002875817055795707, 0.002814302257463044, 0.0026297578624650546, 0.0024605921670502306, 0.002445213467467065, 0.002522106965382894, 0.002722030059964049, 0.002675893961214552  …  0.0015224913756052654, 0.0014609765772726022, 0.0012764321822746128, 0.0012456747831082811, 0.0012302960835251153, 0.0012456747831082811, 0.0012764321822746128, 0.0013071895814409443, 0.0013225682810241101, 0.0013071895814409443], [0, 0, 0, 0, 1])
 ([0.0014763552768557679, 0.0014763552768557679, 0.0014148404785231047, 0.0013071895814409443, 0.0011534025856092865, 0.0009842368901944627, 0.0008458285355100445, 0.0007843137371773813, 0.0009534794325922049, 0.0008304498359268786  …  0.002106882076637418, 0.002137639475803749, 0.002137639475803749, 0.002214532973719578, 0.0022760477720522414, 0.002291426471635407, 0.002229911673302744, 0.002137639475803749, 0.002076124677471086, 0.0020607459778879205], [0, 0, 0, 0, 1])
 ([0.0022760477720522414, 0.002306805171218573, 0.002306805171218573, 0.0022606690724690753, 0.002214532973719578, 0.002229911673302744, 0.0023221838708017387, 0.0024144560683007333, 0.0023375625703849043, 0.0024144560683007333  …  0.0012149173839419497, 0.0010765090876934575, 0.0011380238860261207, 0.0010765090876934575, 0.0011226451864429548, 0.0013225682810241101, 0.001537870075188431, 0.0016455209722705917, 0.0016455209722705917, 0.00161476357310426], [0, 0, 0, 0, 1])
 ([0.0015224913756052654, 0.0019223375647675757, 0.0017224144701864204, 0.001691657071020089, 0.0016762783714369231, 0.0015840061739379285, 0.0017993079681022494, 0.0015993848735210941, 0.0019838524799720915, 0.0025682430641323912  …  0.002368319969551236, 0.0015071126760220995, 0.0006305267413457235, 0.0006766628400952208, 0.0004152249179634393, 0.0003229527204644446, 0.00029219532129811304, 0.00036908881921394196, 0.0004613610167129367, 9.227220480348549e-5], [1, 0, 0, 0, 0])
 ([0.0016608996718537573, 0.0016301422726874258, 0.0015840061739379285, 0.001537870075188431, 0.0014763552768557679, 0.0014148404785231047, 0.0013687043797736074, 0.0013533256801904416, 0.0013071895814409443, 0.0013687043797736074  …  0.0014917339764389337, 0.0016608996718537573, 0.001753171869352752, 0.0017070357706032548, 0.00161476357310426, 0.0015840061739379285, 0.0016301422726874258, 0.0016455209722705917, 0.001768550568935918, 0.001999231179555257], [0, 0, 1, 0, 0])
 ([0.0006766628400952208, 0.0006612841405120551, 0.0005382545438467288, 0.0005843906425962261, 0.0010149942893607944, 0.0014148404785231047, 0.0015224913756052654, 0.0014763552768557679, 0.001399461778939939, 0.0014609765772726022  …  0.0010611303881102917, 0.0010611303881102917, 0.0011226451864429548, 0.0010765090876934575, 0.0010765090876934575, 0.001107266486859789, 0.0010611303881102917, 0.0009688581321753708, 0.0009688581321753708, 0.0010303729889439602], [0, 1, 0, 0, 0])

julia>

julia> # Definir el optimizador inicial

julia> initial_lr = 0.001
0.001

julia> optimizer = Flux.Optimise.ADAM(initial_lr)
Adam(0.001, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())

julia>

julia> # Entrenar el modelo con reducción manual del learning rate

julia> for epoch in 1:20
           if epoch % 5 == 0  # Cada 5 épocas
               current_lr = initial_lr * (0.5 ^ (epoch ÷ 10))  # Reducir el learning rate
               optimizer = Flux.Optimise.ADAM(current_lr)
           end

           Flux.train!(loss, Flux.params(model), train_data, optimizer)
           train_loss = loss(train_x_data, train_y_data)
           val_loss = loss(val_x_data, val_y_data)
           println("Epoch: $epoch - Train Loss: $train_loss - Validation Loss: $val_loss")
           end
Epoch: 1 - Train Loss: 1.5747828 - Validation Loss: 1.5833137
Epoch: 2 - Train Loss: 1.5335469 - Validation Loss: 1.5486244
Epoch: 3 - Train Loss: 1.4603162 - Validation Loss: 1.4945781
Epoch: 4 - Train Loss: 1.4120035 - Validation Loss: 1.450579
Epoch: 5 - Train Loss: 1.3697547 - Validation Loss: 1.4162152
Epoch: 6 - Train Loss: 1.3515576 - Validation Loss: 1.4019529
Epoch: 7 - Train Loss: 1.3717405 - Validation Loss: 1.4167432
Epoch: 8 - Train Loss: 1.2998023 - Validation Loss: 1.3586283
Epoch: 9 - Train Loss: 1.2860211 - Validation Loss: 1.3447593
Epoch: 10 - Train Loss: 1.2631583 - Validation Loss: 1.3256046
Epoch: 11 - Train Loss: 1.2939245 - Validation Loss: 1.3461447
Epoch: 12 - Train Loss: 1.2844046 - Validation Loss: 1.3380679
Epoch: 13 - Train Loss: 1.2510562 - Validation Loss: 1.3081857
Epoch: 14 - Train Loss: 1.2615311 - Validation Loss: 1.3185508
Epoch: 15 - Train Loss: 1.2472011 - Validation Loss: 1.2979769
Epoch: 16 - Train Loss: 1.2371712 - Validation Loss: 1.2876973
Epoch: 17 - Train Loss: 1.2160797 - Validation Loss: 1.2763355
Epoch: 18 - Train Loss: 1.2178351 - Validation Loss: 1.2796903
Epoch: 19 - Train Loss: 1.2247989 - Validation Loss: 1.275755
Epoch: 20 - Train Loss: 1.2285084 - Validation Loss: 1.2906204

julia>

julia> # Evaluar el modelo en el conjunto de validación

julia> function accuracy(x, y)
           predictions = Flux.onecold(model(x))
           labels = Flux.onecold(y)
           mean(predictions .== labels)
       end
accuracy (generic function with 1 method)

julia>

julia> println("Validation Accuracy: ", accuracy(val_x_data, val_y_data))
Validation Accuracy: 0.39

julia>

julia> function predict_image(img_path::String)
           # Verificar si el archivo existe
           if !isfile(img_path)
               error("No file exists at given path: $img_path")
           end

           # Cargar y procesar la imagen
           img = load(img_path)
           img_gray = Gray.(img)
           img_resized = imresize(img_gray, (128, 128))  # Tamaño cambiado a 128x128
           img_array = Float32.(img_resized) / 255.0  # Normalización de 0 a 1
           img_vector = reshape(img_array, 128*128)

           # Hacer la predicción
           prediction = model(img_vector)
               predicted_class = classes[Flux.onecold(prediction)]
           return predicted_class
       end
predict_image (generic function with 1 method)

julia>

julia> # Prueba con una imagen de ejemplo

julia> test_img_path = "C:\\Users\\Jhamil\\Desktop\\Dataset2mil - copia\\AmorSeco\\1bd631a03aff5b854b6a3b1edf0f26efbda6ee55_center_mirror_vertical.jpg"
"C:\\Users\\Jhamil\\Desktop\\Dataset2mil - copia\\AmorSeco\\1bd631a03aff5b854b6a3b1edf0f26efbda6ee55_center_mirror_vertical.jpg"

julia> println("Predicted Class: ", predict_image(test_img_path))
Predicted Class: Perejil

julia>

julia>